{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bilingual_Product_Label_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Bilingual (Chinese + English) Product Label Classification\n",
        "\n",
        "\n",
        "\n",
        "This practical project was deploying machine learning (ML) techniques in natural language processing (NLP) to classify the products in retail stores into pre-defined product categories based on their product labels presented with an integration of Chinese and English characters. There were in total 23 categories ranging from food and beverages to personal healthcare products. During the preprocessing of texts, the following workflow has been deployed:\n",
        "<ol>\n",
        "<li>separately extracted Chinese and English characters</li>\n",
        "<li>applied Jieba pre-built Hidden Markov Model for tokenization of Chinese portions</li>\n",
        "<li>applied NLTK tokenizer for the tokenization of English portions</li>\n",
        "<li>Feature Engineering:\n",
        "<ul><li>One-Hot encoding on the 23-class product categories</li>\n",
        "    <li>Word2Vec embedding training on inputted texts</li>\n",
        "    <li>Encoder dictionary and 3-dimensional encoder sequence preparation based on inputted texts</li></ul>\n",
        "</li>\n",
        "</ol>\n",
        "\n",
        "For the modelling part, two designs with different encoding methods were proposed:\n",
        "<ul>\n",
        "    <li>Concatenate Bi-directional LSTM embeddings from encoder sequence and Word2Vec embeddings</li>\n",
        "    <li>Transformer-style encodings with positional embeddings on sequence lengths and token sizes</li>\n",
        "</ul>\n",
        "\n",
        "Technically, the Transformer encoding methods would grow the number of parameters and indeed it achieved slightly better results than the Bi-LSTM and Word2Vec embeddings, however, for this task the results eventually turned out that the difference of performances of the two models was not significant judging by the multi-class accuracy, probably the task was not sufficient complex to resolve the powers between the two models, or the advantage of Transformer structure. The comparison of classification precision, recall and F1-score of the two sets of results per category could be found at the end of this notebook."
      ],
      "metadata": {
        "id": "7PSmRh-HYy0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U8FWy6O3NUSv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(F'/content/drive/My Drive/Colab Notebooks/NLP/product label classification/df_prod_cat.xlsx')"
      ],
      "metadata": {
        "id": "-DENBLr2OKEa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data = data[pd.notnull(data['product_cat'])]\n",
        "unlabel_data = data[pd.isnull(data['product_cat'])]"
      ],
      "metadata": {
        "id": "WPdR3lCMUuK1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['product_name'].values.tolist()[0:10]"
      ],
      "metadata": {
        "id": "6Ual-YSyVaIe",
        "outputId": "d4bbd0be-1780-413e-8200-be02cf7703df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['奇亞籽生機蘇打餅-蕎麥紫菜250g',\n",
              " '奇亞籽生機蘇打餅-黑芝麻養生250g',\n",
              " '奇亞籽生機蘇打餅-黑椒岩鹽245g',\n",
              " 'SILICON AIR CUSHION 42CMX 42CM X5CM  #YWON-00029',\n",
              " 'YT 一次性尿袋 2000ml',\n",
              " 'Strawberry Yoghurt Drops 9g 士多啤梨味乳酪片',\n",
              " 'Mixed Berry Yoghurt Drops 9g 雜莓味乳酪片',\n",
              " '瑜珈運動地墊 (6mm厚)',\n",
              " '聖誕禮袋-433',\n",
              " '聖誕禮籃-510']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_data.shape)\n",
        "print(unlabel_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itcrp1kIZ_Fk",
        "outputId": "b5a44cde-6dd9-422d-fad5-f164330b809b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7512, 3)\n",
            "(6645, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk import corpus\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivaft8kFaBEN",
        "outputId": "5af22206-495a-414f-8365-ee82ca17fde3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Jieba for tokenization of Chinese texts\n",
        "import jieba"
      ],
      "metadata": {
        "id": "5lTSDNXWaZZk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define codec ranges corresponding to Chinese characters\n",
        "cjk_ranges = [\n",
        "        ( 0x4E00,  0x62FF),\n",
        "        ( 0x6300,  0x77FF),\n",
        "        ( 0x7800,  0x8CFF),\n",
        "        ( 0x8D00,  0x9FCC),\n",
        "        ( 0x3400,  0x4DB5),\n",
        "        (0x20000, 0x215FF),\n",
        "        (0x21600, 0x230FF),\n",
        "        (0x23100, 0x245FF),\n",
        "        (0x24600, 0x260FF),\n",
        "        (0x26100, 0x275FF),\n",
        "        (0x27600, 0x290FF),\n",
        "        (0x29100, 0x2A6DF),\n",
        "        (0x2A700, 0x2B734),\n",
        "        (0x2B740, 0x2B81D),\n",
        "        (0x2B820, 0x2CEAF),\n",
        "        (0x2CEB0, 0x2EBEF),\n",
        "        (0x2F800, 0x2FA1F)\n",
        "    ]\n",
        "\n",
        "## separator of label texts into Chinese and English tokens\n",
        "def chi_eng_extractor(string):\n",
        "\n",
        "    string = re.sub(r'\\d+|[!?@#%^&*\\[\\]\\\\(){}<>]|[.]|[/]|[$]|[-;:,`~=_+]', ' ', string)\n",
        "\n",
        "    es = list()\n",
        "    cs = list()\n",
        "\n",
        "    for i in range(len(string)):\n",
        "        char = ord(string[i])\n",
        "        counter = 0\n",
        "        for bottom, top in cjk_ranges:\n",
        "            if char >= bottom and char <= top:\n",
        "                counter += 1\n",
        "        if counter > 0:\n",
        "            cs.append(string[i])\n",
        "            es.append(\" \")\n",
        "        else:\n",
        "            cs.append(\" \")\n",
        "            es.append(string[i])\n",
        "\n",
        "    es = ''.join(es)\n",
        "    cs = ''.join(cs)\n",
        "    es = es.strip()\n",
        "    es = es.lower()\n",
        "    cs = cs.strip()\n",
        "\n",
        "    es = re.sub(\" +\", \" \", es)\n",
        "    cs = re.sub(\" +\", \" \", cs)\n",
        "\n",
        "    return es, cs\n",
        "\n",
        "## implementation\n",
        "training_label_list = [(chi_eng_extractor(x)) for x in label_data['product_name']]\n",
        "testing_label_list = [(chi_eng_extractor(x)) for x in unlabel_data['product_name']]"
      ],
      "metadata": {
        "id": "nVWIxfhOaEhD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Jieba Chinese tokenization and NLTK English tokenization\n",
        "token_train_en = [word_tokenize(x[0]) for x in training_label_list]\n",
        "token_train_chi = [jieba.lcut(x[1], cut_all=False, HMM=True) for x in training_label_list]\n",
        "token_test_en = [word_tokenize(x[0]) for x in testing_label_list]\n",
        "token_test_chi = [jieba.lcut(x[1], cut_all=False, HMM=True) for x in testing_label_list]"
      ],
      "metadata": {
        "id": "M6aD08t3ac18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8bfa7ff-d9dd-4d49-d3d7-2ca3eb712145"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.800 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## re-concatenate token lists\n",
        "token_train_list = []\n",
        "token_test_list = []\n",
        "for x in range(len(token_train_chi)):\n",
        "    chi_text = [w for w in token_train_chi[x] if w != ' ']\n",
        "    token_train_list.append(token_train_en[x] + chi_text)\n",
        "for x in range(len(token_test_chi)):\n",
        "    chi_text = [w for w in token_test_chi[x] if w != ' ']\n",
        "    token_test_list.append(token_test_en[x] + chi_text)"
      ],
      "metadata": {
        "id": "28BKpI07aeas"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_train_list[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i-WNgrUaues",
        "outputId": "d39adb00-06b4-4c6c-fa09-489315a88040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['g', '奇亞籽生', '機蘇', '打餅', '蕎麥', '紫菜'],\n",
              " ['g', '奇亞籽生', '機蘇', '打餅', '黑芝麻', '養生'],\n",
              " ['g', '奇亞籽生', '機蘇', '打餅', '黑椒', '岩鹽'],\n",
              " ['silicon', 'air', 'cushion', 'cmx', 'cm', 'x', 'cm', 'ywon'],\n",
              " ['yt', 'ml', '一次性', '尿袋'],\n",
              " ['strawberry', 'yoghurt', 'drops', 'g', '士多啤梨', '味', '乳酪', '片'],\n",
              " ['mixed', 'berry', 'yoghurt', 'drops', 'g', '雜莓味', '乳酪', '片'],\n",
              " ['mm', '瑜珈', '運動', '地', '墊', '厚'],\n",
              " ['聖誕禮袋'],\n",
              " ['聖誕禮籃']]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "fit_onehot = OneHotEncoder().fit(label_data[['product_cat']])\n",
        "target = fit_onehot.transform(label_data[['product_cat']]).toarray()\n",
        "cat_labels = fit_onehot.categories_"
      ],
      "metadata": {
        "id": "w7XPhl3BRNKW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[{v:i} for v,i in enumerate(cat_labels[0].tolist())]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6yqKcIVGjKs",
        "outputId": "7139d8c0-77b2-45f4-c48d-ed07743affba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Fine Food / Snacks',\n",
              " 1: 'Healthcare Products',\n",
              " 2: 'Drinks',\n",
              " 3: 'Sports Equipments',\n",
              " 4: 'Personal Hygene Products',\n",
              " 5: 'Nursing Supplies',\n",
              " 6: 'Books',\n",
              " 7: 'Baby Products',\n",
              " 8: 'Oils',\n",
              " 9: 'Bathroom Accessories',\n",
              " 10: 'Skin Care Products',\n",
              " 11: 'Water Strainers',\n",
              " 12: 'Nutritional Supplements',\n",
              " 13: 'Physiotherapy Equipments',\n",
              " 14: 'Pharmacy',\n",
              " 15: 'Gifts',\n",
              " 16: 'Staples',\n",
              " 17: 'Organic Food',\n",
              " 18: 'Health Monitoring Devices',\n",
              " 19: 'Seasoning',\n",
              " 20: 'Wheelchairs',\n",
              " 21: 'Masks',\n",
              " 22: 'Electronics'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Bidirectional, Dense\n",
        "from tensorflow.keras.layers import TimeDistributed, Dropout, Activation, Concatenate, Dot\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "kryBTpC1fgbr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "txpoT-aaWjIL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define encoder\n",
        "def process_seq2seq_encoder_input(encoder):\n",
        "    ## convert into dictionary\n",
        "    reserved = {'<PAD>': 0, '<UNK>': 1}\n",
        "    enc_list = [w for i in encoder for w in i]\n",
        "    enc_dict = {e:i+2 for i,e in enumerate(set(enc_list))}\n",
        "    enc_dict = {**reserved, **enc_dict}\n",
        "    ## reserved key-index for padding sequence length, out-of-dictionary words\n",
        "    enc_seq = []\n",
        "    for e in range(len(encoder)):\n",
        "        enc_sub_seq = []\n",
        "        for se in encoder[e]:\n",
        "            enc_sub_seq.append(enc_dict.get(se))\n",
        "        enc_seq.append(enc_sub_seq)\n",
        "    ## padding sequence\n",
        "    MAX_LEN = max([len(x) for x in enc_seq])\n",
        "    padded_seq = pad_sequences(enc_seq, maxlen=MAX_LEN, padding='post')\n",
        "    ## one-hot vectors\n",
        "    enc_seq_cat = to_categorical(padded_seq, num_classes=len(enc_dict))\n",
        "    return enc_dict, enc_seq, padded_seq, enc_seq_cat"
      ],
      "metadata": {
        "id": "gRHZSuIBpirU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_dict, encoder_seq, pad_encode_seq, encoder_seq_cat = process_seq2seq_encoder_input(token_train_list)"
      ],
      "metadata": {
        "id": "2oyWzqTrqBG7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_seq_cat.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB_iosSsqohS",
        "outputId": "4b008d33-8580-46c7-e1e0-cb4a4827f3d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7512, 27, 6522)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyqzNedCy6M5",
        "outputId": "cac50f9b-96e6-42c4-e6bb-d9700d9d6098"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7512, 23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYyPwAvaHhJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803e9981-450e-482c-8b2b-f2314ad4b115"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "w2v = word2vec.Word2Vec(token_train_list, size=200, window=3, min_count=1, seed=42)\n",
        "w2v.train(token_train_list, total_examples=len(token_train_list), epochs=1000)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40218928, 46661000)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2Xg-bpOxuwg"
      },
      "source": [
        "## extract trained word vectors\n",
        "word_vec_array = np.zeros((pad_encode_seq.shape[0], pad_encode_seq.shape[1], 200))\n",
        "\n",
        "for x in range(len(pad_encode_seq)):\n",
        "  for y in range(len(pad_encode_seq[x])):\n",
        "    if pad_encode_seq[x][y] == 0:\n",
        "      word_vec_array[x][y] = np.zeros((200,))\n",
        "    elif pad_encode_seq[x][y] == 1:\n",
        "      word_vec_array[x][y] = np.mean([w2v[word] for word in w2v.wv.vocab], axis=0)\n",
        "    else:\n",
        "      word_vec_array[x][y] = w2v[list(encoder_dict.keys())[list(encoder_dict.values())[encoder_seq[x][y]]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "encoder_seq_cat_train, encoder_seq_cat_test, w2v_arr_train, w2v_arr_test, pad_encode_seq_train, pad_encode_seq_test, y_train, y_test = \\\n",
        "train_test_split(encoder_seq_cat, word_vec_array, pad_encode_seq, target,\n",
        "                 stratify = label_data[['product_cat']], test_size = 0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "mroc_X2rpxdJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del encoder_seq_cat\n",
        "del pad_encode_seq\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "vZm0287usACX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ-Sj21iDXIv"
      },
      "source": [
        "def Word2Vec_Seq2Label(encoder_dict, encoder_seq):\n",
        "\n",
        "    len_en = len(encoder_dict)\n",
        "    max_length_en = max([len(x) for x in encoder_seq])\n",
        "\n",
        "    encoder_inputs = Input(shape=(None, len_en))\n",
        "    encoder_LSTM = Bidirectional(LSTM(400, return_state=True, return_sequences=True))\n",
        "    encoder_hidden_vec, forward_last_h, forward_last_c, backward_last_h, backward_last_c = encoder_LSTM(encoder_inputs)\n",
        "    enc_state_last_h = Concatenate()([forward_last_h, backward_last_h])\n",
        "    enc_state_last_c = Concatenate()([forward_last_c, backward_last_c])\n",
        "\n",
        "    w2v_encoder_inputs = Input(shape=(None, 200))\n",
        "    w2v_LSTM = LSTM(200, return_sequences=False)\n",
        "    w2v_LSTM_layer = w2v_LSTM(w2v_encoder_inputs)\n",
        "\n",
        "    concat_layer = Concatenate()([enc_state_last_h, enc_state_last_c, w2v_LSTM_layer])\n",
        "    \n",
        "    dense_1 = Dense(1800, activation='relu')(concat_layer)\n",
        "    dense_2 = Dense(200, activation='relu')(dense_1)\n",
        "    dense_3 = Dense(50, activation='relu')(dense_2)\n",
        "    outputs = Dense(len(cat_labels[0]), activation='softmax')(dense_3)\n",
        "\n",
        "    model = Model([encoder_inputs, w2v_encoder_inputs], outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpler_model = Word2Vec_Seq2Label(encoder_dict, pad_encode_seq)\n",
        "simpler_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "5TsvGeK_YYGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simpler_model.summary(line_length=125)"
      ],
      "metadata": {
        "id": "7tPVRXqVYYmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012122bd-a029-4cbf-b602-6c9778de9949"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_____________________________________________________________________________________________________________________________\n",
            " Layer (type)                            Output Shape               Param #        Connected to                              \n",
            "=============================================================================================================================\n",
            " input_1 (InputLayer)                    [(None, None, 6522)]       0              []                                        \n",
            "                                                                                                                             \n",
            " bidirectional (Bidirectional)           [(None, None, 800),        22153600       ['input_1[0][0]']                         \n",
            "                                          (None, 400),                                                                       \n",
            "                                          (None, 400),                                                                       \n",
            "                                          (None, 400),                                                                       \n",
            "                                          (None, 400)]                                                                       \n",
            "                                                                                                                             \n",
            " input_2 (InputLayer)                    [(None, None, 200)]        0              []                                        \n",
            "                                                                                                                             \n",
            " concatenate (Concatenate)               (None, 800)                0              ['bidirectional[0][1]',                   \n",
            "                                                                                    'bidirectional[0][3]']                   \n",
            "                                                                                                                             \n",
            " concatenate_1 (Concatenate)             (None, 800)                0              ['bidirectional[0][2]',                   \n",
            "                                                                                    'bidirectional[0][4]']                   \n",
            "                                                                                                                             \n",
            " lstm_1 (LSTM)                           (None, 200)                320800         ['input_2[0][0]']                         \n",
            "                                                                                                                             \n",
            " concatenate_2 (Concatenate)             (None, 1800)               0              ['concatenate[0][0]',                     \n",
            "                                                                                    'concatenate_1[0][0]',                   \n",
            "                                                                                    'lstm_1[0][0]']                          \n",
            "                                                                                                                             \n",
            " dense (Dense)                           (None, 1800)               3241800        ['concatenate_2[0][0]']                   \n",
            "                                                                                                                             \n",
            " dense_1 (Dense)                         (None, 200)                360200         ['dense[0][0]']                           \n",
            "                                                                                                                             \n",
            " dense_2 (Dense)                         (None, 50)                 10050          ['dense_1[0][0]']                         \n",
            "                                                                                                                             \n",
            " dense_3 (Dense)                         (None, 23)                 1173           ['dense_2[0][0]']                         \n",
            "                                                                                                                             \n",
            "=============================================================================================================================\n",
            "Total params: 26,087,623\n",
            "Trainable params: 26,087,623\n",
            "Non-trainable params: 0\n",
            "_____________________________________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpler_model.fit([encoder_seq_cat_train, w2v_arr_train], y_train,\n",
        "                  validation_split = 0.2, batch_size=8, epochs=12, \n",
        "                  callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmPXfIBTBM7s",
        "outputId": "da757077-76f2-443c-e03a-73762074c200"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "676/676 [==============================] - 25s 27ms/step - loss: 1.9186 - acc: 0.4469 - val_loss: 1.2539 - val_acc: 0.6287 - lr: 1.0000e-04\n",
            "Epoch 2/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.8896 - acc: 0.7398 - val_loss: 0.9089 - val_acc: 0.7419 - lr: 1.0000e-04\n",
            "Epoch 3/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.5491 - acc: 0.8393 - val_loss: 0.7377 - val_acc: 0.7803 - lr: 1.0000e-04\n",
            "Epoch 4/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.3598 - acc: 0.8935 - val_loss: 0.7442 - val_acc: 0.7959 - lr: 1.0000e-04\n",
            "Epoch 5/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.2495 - acc: 0.9283 - val_loss: 0.7835 - val_acc: 0.8025 - lr: 1.0000e-04\n",
            "Epoch 6/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.1861 - acc: 0.9467 - val_loss: 0.8270 - val_acc: 0.8129 - lr: 1.0000e-04\n",
            "Epoch 7/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.1446 - acc: 0.9560 - val_loss: 0.8962 - val_acc: 0.8018 - lr: 1.0000e-04\n",
            "Epoch 8/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.0900 - acc: 0.9747 - val_loss: 0.8163 - val_acc: 0.8121 - lr: 5.0000e-05\n",
            "Epoch 9/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.0722 - acc: 0.9793 - val_loss: 0.9712 - val_acc: 0.8092 - lr: 5.0000e-05\n",
            "Epoch 10/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.0655 - acc: 0.9813 - val_loss: 0.9564 - val_acc: 0.8025 - lr: 5.0000e-05\n",
            "Epoch 11/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.0619 - acc: 0.9832 - val_loss: 0.9113 - val_acc: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 12/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.0445 - acc: 0.9872 - val_loss: 0.9647 - val_acc: 0.8121 - lr: 2.5000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe7947c9710>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## make predictions\n",
        "pred_y_test = simpler_model.predict([encoder_seq_cat_test, w2v_arr_test])\n",
        "pred_y_test_class = np.argmax(pred_y_test, axis=1)"
      ],
      "metadata": {
        "id": "JZbJeRCwZ2cO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(y_test, axis=1), pred_y_test_class))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mesW6c9tGxr",
        "outputId": "6e8f4242-56e6-4eb7-d0f7-dc9a1353a8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91       115\n",
            "           1       0.73      0.66      0.69        62\n",
            "           2       0.89      0.90      0.89        70\n",
            "           3       0.75      0.80      0.77        15\n",
            "           4       0.98      0.94      0.96        47\n",
            "           5       0.91      0.93      0.92       144\n",
            "           6       0.00      0.00      0.00         2\n",
            "           7       0.50      0.50      0.50         2\n",
            "           8       0.50      0.40      0.44         5\n",
            "           9       0.61      0.76      0.67        41\n",
            "          10       0.82      0.65      0.72        48\n",
            "          11       1.00      0.86      0.92         7\n",
            "          12       0.90      0.76      0.83        25\n",
            "          13       0.67      0.73      0.70        45\n",
            "          14       0.75      0.75      0.75         4\n",
            "          15       0.60      1.00      0.75         3\n",
            "          16       1.00      0.80      0.89        20\n",
            "          17       0.91      0.91      0.91        23\n",
            "          18       0.71      0.83      0.77         6\n",
            "          19       0.67      0.91      0.77        11\n",
            "          20       0.83      0.50      0.62        10\n",
            "          21       0.79      0.88      0.83        25\n",
            "          22       0.95      0.86      0.90        22\n",
            "\n",
            "    accuracy                           0.84       752\n",
            "   macro avg       0.75      0.75      0.75       752\n",
            "weighted avg       0.84      0.84      0.83       752\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.models.Sequential(\n",
        "            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"), \n",
        "             tf.keras.layers.Dense(embed_dim),\n",
        "             ]\n",
        "        )\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        \n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "pVauoyhuff_8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "        \n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ],
      "metadata": {
        "id": "0s7eWxInjqJc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Transformer_Seq2Label():\n",
        "\n",
        "    embed_dim = len(encoder_dict) // 8\n",
        "    latent_dim = len(encoder_dict) // 8 * 2\n",
        "    num_heads = 8\n",
        "    num_cats = len(cat_labels[0])\n",
        "    vocab_size = len(encoder_dict)\n",
        "    sequence_length = pad_encode_seq_train.shape[1]\n",
        "\n",
        "    encoder_inputs = tf.keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "    encoder_x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "    encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(encoder_x)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(0.5)(encoder_outputs)\n",
        "    x = tf.keras.layers.Reshape(target_shape=(sequence_length * embed_dim,))(x)\n",
        "    x = tf.keras.layers.Dense(4000, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(800, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(num_cats, activation='softmax')(x)\n",
        "\n",
        "    transformer = tf.keras.models.Model(encoder_inputs, outputs, name=\"transformer\")\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "rw3u_1RWj4WU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2label = Transformer_Seq2Label()\n",
        "seq2label.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "gjvHrlo20bEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2label.summary(line_length=125)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDtj8WXbfH7j",
        "outputId": "0eb1e353-46e3-40d7-ca92-1e351b128be0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "_____________________________________________________________________________________________________________________________\n",
            " Layer (type)                                           Output Shape                                      Param #            \n",
            "=============================================================================================================================\n",
            " encoder_inputs (InputLayer)                            [(None, None)]                                    0                  \n",
            "                                                                                                                             \n",
            " positional_embedding (PositionalEmbedding)             (None, None, 815)                                 5337435            \n",
            "                                                                                                                             \n",
            " transformer_encoder (TransformerEncoder)               (None, None, 815)                                 23938180           \n",
            "                                                                                                                             \n",
            " dropout (Dropout)                                      (None, None, 815)                                 0                  \n",
            "                                                                                                                             \n",
            " reshape (Reshape)                                      (None, 22005)                                     0                  \n",
            "                                                                                                                             \n",
            " dense_6 (Dense)                                        (None, 4000)                                      88024000           \n",
            "                                                                                                                             \n",
            " dense_7 (Dense)                                        (None, 800)                                       3200800            \n",
            "                                                                                                                             \n",
            " dense_8 (Dense)                                        (None, 200)                                       160200             \n",
            "                                                                                                                             \n",
            " dense_9 (Dense)                                        (None, 50)                                        10050              \n",
            "                                                                                                                             \n",
            " dense_10 (Dense)                                       (None, 23)                                        1173               \n",
            "                                                                                                                             \n",
            "=============================================================================================================================\n",
            "Total params: 120,671,838\n",
            "Trainable params: 120,671,838\n",
            "Non-trainable params: 0\n",
            "_____________________________________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq2label.fit(pad_encode_seq_train, y_train, validation_split=0.2, batch_size=8, epochs=12,\n",
        "              callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8rBSHx-tf0U",
        "outputId": "0fe3782c-5b6e-4580-9a61-da5a09c2bfc1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "676/676 [==============================] - 18s 24ms/step - loss: 2.5982 - acc: 0.2419 - val_loss: 1.9061 - val_acc: 0.4941 - lr: 1.0000e-04\n",
            "Epoch 2/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 1.2941 - acc: 0.6354 - val_loss: 1.0812 - val_acc: 0.6864 - lr: 1.0000e-04\n",
            "Epoch 3/12\n",
            "676/676 [==============================] - 16s 23ms/step - loss: 0.6137 - acc: 0.8297 - val_loss: 0.8975 - val_acc: 0.7648 - lr: 1.0000e-04\n",
            "Epoch 4/12\n",
            "676/676 [==============================] - 16s 23ms/step - loss: 0.3991 - acc: 0.8891 - val_loss: 0.8893 - val_acc: 0.7973 - lr: 1.0000e-04\n",
            "Epoch 5/12\n",
            "676/676 [==============================] - 16s 23ms/step - loss: 0.2757 - acc: 0.9242 - val_loss: 0.9870 - val_acc: 0.7811 - lr: 1.0000e-04\n",
            "Epoch 6/12\n",
            "676/676 [==============================] - 16s 23ms/step - loss: 0.2233 - acc: 0.9360 - val_loss: 0.8693 - val_acc: 0.8121 - lr: 1.0000e-04\n",
            "Epoch 7/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.1844 - acc: 0.9456 - val_loss: 1.0198 - val_acc: 0.7966 - lr: 1.0000e-04\n",
            "Epoch 8/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.1565 - acc: 0.9525 - val_loss: 1.0841 - val_acc: 0.8062 - lr: 1.0000e-04\n",
            "Epoch 9/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.1310 - acc: 0.9584 - val_loss: 1.1569 - val_acc: 0.8121 - lr: 1.0000e-04\n",
            "Epoch 10/12\n",
            "676/676 [==============================] - 16s 24ms/step - loss: 0.1137 - acc: 0.9651 - val_loss: 1.1540 - val_acc: 0.7877 - lr: 1.0000e-04\n",
            "Epoch 11/12\n",
            "676/676 [==============================] - 16s 23ms/step - loss: 0.0643 - acc: 0.9811 - val_loss: 1.0041 - val_acc: 0.8151 - lr: 5.0000e-05\n",
            "Epoch 12/12\n",
            "676/676 [==============================] - 16s 23ms/step - loss: 0.0438 - acc: 0.9863 - val_loss: 1.1467 - val_acc: 0.8232 - lr: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe77cb2b590>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## make predictions\n",
        "pred_y_test = seq2label.predict(pad_encode_seq_test)\n",
        "pred_y_test_class = np.argmax(pred_y_test, axis=1)"
      ],
      "metadata": {
        "id": "1frK30VjFj2E"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(y_test, axis=1), pred_y_test_class))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfX7X_IgwoU2",
        "outputId": "84e4367d-bdb2-4f0e-faef-e865081853c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.93      0.92       115\n",
            "           1       0.61      0.77      0.68        62\n",
            "           2       0.94      0.89      0.91        70\n",
            "           3       0.80      0.80      0.80        15\n",
            "           4       1.00      0.94      0.97        47\n",
            "           5       0.93      0.95      0.94       144\n",
            "           6       0.50      0.50      0.50         2\n",
            "           7       0.00      0.00      0.00         2\n",
            "           8       0.75      0.60      0.67         5\n",
            "           9       0.64      0.68      0.66        41\n",
            "          10       0.81      0.71      0.76        48\n",
            "          11       1.00      0.86      0.92         7\n",
            "          12       0.92      0.88      0.90        25\n",
            "          13       0.80      0.62      0.70        45\n",
            "          14       0.75      0.75      0.75         4\n",
            "          15       0.75      1.00      0.86         3\n",
            "          16       0.95      0.90      0.92        20\n",
            "          17       0.81      0.96      0.88        23\n",
            "          18       1.00      1.00      1.00         6\n",
            "          19       0.90      0.82      0.86        11\n",
            "          20       0.67      0.60      0.63        10\n",
            "          21       0.79      0.88      0.83        25\n",
            "          22       1.00      0.82      0.90        22\n",
            "\n",
            "    accuracy                           0.85       752\n",
            "   macro avg       0.79      0.78      0.78       752\n",
            "weighted avg       0.85      0.85      0.85       752\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "The Transformer encoding model has attained an accuracy of 85% while the Bi-directional and Word2Vec embedding model also has a slightly lower accuracy of 84%. For individual class performances, 14 out of 23 categories have achieved F1-score over 80% for the Transformer encoding model, major improvements have been observed for \"oils\", \"health monitoring devices\", \"seasoning\", \"gifts\". Relatively, for some categories with smaller proportion of items, like \"books\" and \"baby products\", their performances may require more testing sampels to validate or refine the power of the model on these specific classes. The \"nutritional supplements\" has also boosted recall while maintaining same level of precision, which increases the F1-score to 90%."
      ],
      "metadata": {
        "id": "XAvUxIasWiG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th>ID</td>\n",
        "            <th>Product Category</th>\n",
        "            <th>Support</td>\n",
        "            <th colspan=\"3\">Bi-LSTM + Word2Vec Embeddings</th>\n",
        "            <th colspan=\"3\">Transformer Positional Embeddings</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th></th>\n",
        "            <th></th>\n",
        "            <th></th>\n",
        "            <th>precision</th>\n",
        "            <th>recall</th>\n",
        "            <th>f1-score</th>\n",
        "            <th>precision</th>\n",
        "            <th>recall</th>\n",
        "            <th>f1-score</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>0</td>\n",
        "            <td>Fine Food / Snacks</td>\n",
        "            <td>115</td>\n",
        "            <td>0.89</td>\n",
        "            <td>0.93</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.93</td>\n",
        "            <td>0.92</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>1</td>\n",
        "            <td>Healthcare Products</td>\n",
        "            <td>62</td>\n",
        "            <td>0.73</td>\n",
        "            <td>0.66</td>\n",
        "            <td>0.69</td>\n",
        "            <td>0.61</td>\n",
        "            <td>0.77</td>\n",
        "            <td>0.68</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>2</td>\n",
        "            <td>Drinks</td>\n",
        "            <td>70</td>\n",
        "            <td>0.89</td>\n",
        "            <td>0.90</td>\n",
        "            <td>0.89</td>\n",
        "            <td>0.94</td>\n",
        "            <td>0.89</td>\n",
        "            <td>0.91</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>3</td>\n",
        "            <td>Sports Equipments</td>\n",
        "            <td>15</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.80</td>\n",
        "            <td>0.77</td>\n",
        "            <td>0.80</td>\n",
        "            <td>0.80</td>\n",
        "            <td>0.80</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>4</td>\n",
        "            <td>Personal Hygene Products</td>\n",
        "            <td>47</td>\n",
        "            <td>0.98</td>\n",
        "            <td>0.94</td>\n",
        "            <td>0.96</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.94</td>\n",
        "            <td>0.97</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>5</td>\n",
        "            <td>Nursing Supplies</td>\n",
        "            <td>144</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.93</td>\n",
        "            <td>0.92</td>\n",
        "            <td>0.93</td>\n",
        "            <td>0.95</td>\n",
        "            <td>0.94</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>6</td>\n",
        "            <td>Books</td>\n",
        "            <td>2</td>\n",
        "            <td>0.00</td>\n",
        "            <td>0.00</td>\n",
        "            <td>0.00</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.50</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>7</td>\n",
        "            <td>Baby Products</td>\n",
        "            <td>2</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.00</td>\n",
        "            <td>0.00</td>\n",
        "            <td>0.00</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>8</td>\n",
        "            <td>Oils</td>\n",
        "            <td>5</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.40</td>\n",
        "            <td>0.44</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.60</td>\n",
        "            <td>0.67</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>9</td>\n",
        "            <td>Bathroom Accessories</td>\n",
        "            <td>41</td>\n",
        "            <td>0.61</td>\n",
        "            <td>0.76</td>\n",
        "            <td>0.67</td>\n",
        "            <td>0.64</td>\n",
        "            <td>0.68</td>\n",
        "            <td>0.66</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>10</td>\n",
        "            <td>Skin Care Products</td>\n",
        "            <td>48</td>\n",
        "            <td>0.82</td>\n",
        "            <td>0.65</td>\n",
        "            <td>0.72</td>\n",
        "            <td>0.81</td>\n",
        "            <td>0.71</td>\n",
        "            <td>0.76</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>11</td>\n",
        "            <td>Water Strainers</td>\n",
        "            <td>7</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.86</td>\n",
        "            <td>0.92</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.86</td>\n",
        "            <td>0.92</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>12</td>\n",
        "            <td>Nutritional Supplements</td>\n",
        "            <td>25</td>\n",
        "            <td>0.90</td>\n",
        "            <td>0.76</td>\n",
        "            <td>0.83</td>\n",
        "            <td>0.92</td>\n",
        "            <td>0.88</td>\n",
        "            <td>0.90</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>13</td>\n",
        "            <td>Physiotherapy Equipments</td>\n",
        "            <td>45</td>\n",
        "            <td>0.67</td>\n",
        "            <td>0.73</td>\n",
        "            <td>0.70</td>\n",
        "            <td>0.80</td>\n",
        "            <td>0.62</td>\n",
        "            <td>0.70</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>14</td>\n",
        "            <td>Pharmacy</td>\n",
        "            <td>4</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.75</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>15</td>\n",
        "            <td>Gifts</td>\n",
        "            <td>3</td>\n",
        "            <td>0.60</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.75</td>\n",
        "            <td>0.75</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.86</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>16</td>\n",
        "            <td>Staples</td>\n",
        "            <td>20</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.80</td>\n",
        "            <td>0.89</td>\n",
        "            <td>0.95</td>\n",
        "            <td>0.90</td>\n",
        "            <td>0.92</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>17</td>\n",
        "            <td>Organic Food</td>\n",
        "            <td>23</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.81</td>\n",
        "            <td>0.96</td>\n",
        "            <td>0.88</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>18</td>\n",
        "            <td>Health Monitoring Devices</td>\n",
        "            <td>6</td>\n",
        "            <td>0.71</td>\n",
        "            <td>0.83</td>\n",
        "            <td>0.77</td>\n",
        "            <td>1.00</td>\n",
        "            <td>1.00</td>\n",
        "            <td>1.00</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>19</td>\n",
        "            <td>Seasoning</td>\n",
        "            <td>11</td>\n",
        "            <td>0.67</td>\n",
        "            <td>0.91</td>\n",
        "            <td>0.77</td>\n",
        "            <td>0.90</td>\n",
        "            <td>0.82</td>\n",
        "            <td>0.86</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>20</td>\n",
        "            <td>Wheelchairs</td>\n",
        "            <td>10</td>\n",
        "            <td>0.83</td>\n",
        "            <td>0.50</td>\n",
        "            <td>0.62</td>\n",
        "            <td>0.67</td>\n",
        "            <td>0.60</td>\n",
        "            <td>0.63</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>21</td>\n",
        "            <td>Masks</td>\n",
        "            <td>25</td>\n",
        "            <td>0.79</td>\n",
        "            <td>0.88</td>\n",
        "            <td>0.83</td>\n",
        "            <td>0.79</td>\n",
        "            <td>0.88</td>\n",
        "            <td>0.83</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>22</td>\n",
        "            <td>Electronics</td>\n",
        "            <td>22</td>\n",
        "            <td>0.95</td>\n",
        "            <td>0.86</td>\n",
        "            <td>0.90</td>\n",
        "            <td>1.00</td>\n",
        "            <td>0.82</td>\n",
        "            <td>0.90</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><br></td>\n",
        "            <td><b>accuracy</b></td>\n",
        "            <td><b>752</b></td>\n",
        "            <td><br></td>\n",
        "            <td><br></td>\n",
        "            <td><b>0.84</b></td>\n",
        "            <td><br></td>\n",
        "            <td><br></td>\n",
        "            <td><b>0.85</b></td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><br></td>\n",
        "            <td><b>macro average</b></td>\n",
        "            <td><b>752</b></td>\n",
        "            <td><b>0.75</b></td>\n",
        "            <td><b>0.75</b></td>\n",
        "            <td><b>0.75</b></td>\n",
        "            <td><b>0.79</b></td>\n",
        "            <td><b>0.78</b></td>\n",
        "            <td><b>0.78</b></td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td><br></td>\n",
        "            <td><b>weighted average</b></td>\n",
        "            <td><b>752</b></td>\n",
        "            <td><b>0.84</b></td>\n",
        "            <td><b>0.84</b></td>\n",
        "            <td><b>0.83</b></td>\n",
        "            <td><b>0.85</b></td>\n",
        "            <td><b>0.85</b></td>\n",
        "            <td><b>0.85</b></td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>"
      ],
      "metadata": {
        "id": "HxgIxh5YUXUG"
      }
    }
  ]
}